<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><link href=https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css integrity=sha512-... crossorigin=anonymous referrerpolicy=no-referrer><meta property="og:title" content="The Evaluation of RecSys - Part 3"><meta property="og:description" content="Context: Why This Post Matters, Who It’s For, and What You’ll Learn
Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide & Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements."><meta property="og:type" content="article"><meta property="og:url" content="https://newabdullah.com/posts/the-evaluation-of-recsys-part-3/"><meta property="og:image" content><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="The Evaluation of RecSys - Part 3"><meta name=twitter:description content="Context: Why This Post Matters, Who It’s For, and What You’ll Learn
Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide & Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements."><meta name=twitter:image content><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Evaluation of RecSys - Part 3 | Abdullah Al Mamun</title>
<meta name=keywords content="recommendation systems,deep learning,neural collaborative filtering,wide and deep,deepfm,din,dlrm,multi-task learning"><meta name=description content="Context: Why This Post Matters, Who It’s For, and What You’ll Learn
Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide & Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements."><meta name=author content="Abdullah Al Mamun"><link rel=canonical href=https://newabdullah.com/posts/the-evaluation-of-recsys-part-3/><link crossorigin=anonymous href=/assets/css/stylesheet.c12a1208b6212e62f9164f8e928224324f88ac6630dca9bd54d7c312df35e1eb.css integrity="sha256-wSoSCLYhLmL5Fk+OkoIkMk+IrGYw3Km9VNfDEt814es=" rel="preload stylesheet" as=style><link rel=icon href=https://newabdullah.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://newabdullah.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://newabdullah.com/favicon-32x32.png><link rel=apple-touch-icon href=https://newabdullah.com/apple-touch-icon.png><link rel=mask-icon href=https://newabdullah.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://newabdullah.com/posts/the-evaluation-of-recsys-part-3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://newabdullah.com/posts/the-evaluation-of-recsys-part-3/"><meta property="og:site_name" content="Abdullah Al Mamun"><meta property="og:title" content="The Evaluation of RecSys - Part 3"><meta property="og:description" content="Context: Why This Post Matters, Who It’s For, and What You’ll Learn Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide & Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-12T00:00:00+00:00"><meta property="article:tag" content="Recommendation Systems"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Neural Collaborative Filtering"><meta property="article:tag" content="Wide and Deep"><meta property="article:tag" content="Deepfm"><meta property="article:tag" content="Din"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Evaluation of RecSys - Part 3"><meta name=twitter:description content="Context: Why This Post Matters, Who It’s For, and What You’ll Learn
Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide & Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://newabdullah.com/posts/"},{"@type":"ListItem","position":2,"name":"The Evaluation of RecSys - Part 3","item":"https://newabdullah.com/posts/the-evaluation-of-recsys-part-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Evaluation of RecSys - Part 3","name":"The Evaluation of RecSys - Part 3","description":"Context: Why This Post Matters, Who It’s For, and What You’ll Learn Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide \u0026amp; Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements.\n","keywords":["recommendation systems","deep learning","neural collaborative filtering","wide and deep","deepfm","din","dlrm","multi-task learning"],"articleBody":"Context: Why This Post Matters, Who It’s For, and What You’ll Learn Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide \u0026 Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements.\nRecap: Where We Left Off In Part 2, we saw how FM extended MF by modeling pairwise feature interactions, making it a powerhouse for sparse settings like click-through rate (CTR) prediction. Its prediction function, ( \\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j ), captured second-order relationships efficiently but couldn’t handle higher-order interactions or non-linear patterns beyond its linear assumptions. XGBoost, meanwhile, leveraged tree ensembles to rank items based on non-linear feature combinations, shining in tasks like top-N recommendations. Yet, it struggled with high-dimensional sparse data and required extensive manual feature engineering, limiting its scalability. These gaps—missing deep non-linearities, higher-order interactions, and sequential modeling—paved the way for DNNs, which, starting in 2016, redefined RecSys by learning complex patterns directly from raw data.\nThe Big Picture: The Deep Learning Revolution in RecSys Picture a recommendation system as a guide helping you navigate a vast library. In Part 2, our guide used simple rules: FM paired clues like your reading history with book traits, while XGBoost ranked options by studying everyone’s preferences. But what if your interests shift over time (say, from mysteries to sci-fi), or the guide needs to predict both what you’ll read and whether you’ll buy it? These earlier methods faltered. DNNs emerged as a smarter guide, capable of deciphering intricate patterns, tracking sequential behaviors, and juggling multiple goals. From 2016’s Wide \u0026 Deep to 2023’s AdaTT, this era saw RecSys evolve to handle complex user behaviors with unprecedented accuracy, shaping modern systems in companies like Google, Alibaba, and Facebook.\nDeep Dive: The Evolution of DNNs in RecSys Let’s explore this journey, starting with Neural Collaborative Filtering, which kicked off the DNN era by rethinking how we model user-item interactions.\nNeural Collaborative Filtering (NCF, 2017) Traditional MF, a staple from Part 1, predicts user-item interactions via a dot product: ( \\hat{r}_{ui} = p_u^T q_i ), where ( p_u ) and ( q_i ) are latent vectors for user ( u ) and item ( i ). This worked well for explicit ratings but assumed linearity, missing non-linear patterns in implicit feedback like clicks or views. In 2017, He et al. proposed Neural Collaborative Filtering (NCF) to overcome this, replacing the dot product with a neural network to capture complex, non-linear relationships. The motivation was clear: real-world preferences aren’t linear—liking sci-fi movies doesn’t linearly predict liking sci-fi books—and DNNs, fresh from successes in vision and NLP, offered a way to model these nuances.\nNCF’s architecture comes in three flavors. First, the inputs are simple: one-hot encoded user ID ( \\mathbf{u} ) and item ID ( \\mathbf{i} ), mapped to dense embeddings ( \\mathbf{p}_u ) and ( \\mathbf{q}_i \\in \\mathbb{R}^{32} ) via lookup tables. The Generalized Matrix Factorization (GMF) variant mimics MF but with a neural twist: it computes an element-wise product ( \\mathbf{p}_u \\odot \\mathbf{q}i ), feeds it through a linear layer with weights ( \\mathbf{w} ), and applies a sigmoid activation to output a probability: ( \\hat{y}{ui} = \\sigma(\\mathbf{w}^T (\\mathbf{p}_u \\odot \\mathbf{q}_i)) ). This retains MF’s linear interaction but learns the weighting neurally. The Multi-Layer Perceptron (MLP) variant takes a different tack, concatenating the embeddings into ( [\\mathbf{p}_u, \\mathbf{q}_i] ) and passing them through three fully connected layers (e.g., 256, 128, 64 neurons) with ReLU activations: ( \\mathbf{z}_1 = \\text{ReLU}(\\mathbf{W}_1 [\\mathbf{p}u, \\mathbf{q}i] + \\mathbf{b}1) ), followed by more layers, ending in a prediction layer. This captures non-linear interactions unavailable to MF. Finally, Neural Matrix Factorization (NeuMF) combines both, concatenating GMF’s and MLP’s penultimate outputs and applying a final linear layer: ( \\hat{y}{ui} = \\sigma(\\mathbf{w}^T [\\mathbf{z}{\\text{GMF}}, \\mathbf{z}{\\text{MLP}}]) ). This hybrid leverages both linear and non-linear modeling.\nFor implicit feedback (e.g., clicks), NCF uses binary cross-entropy as its loss: ( L = -\\sum_{(u,i) \\in D} [y_{ui} \\log(\\hat{y}{ui}) + (1-y{ui}) \\log(1-\\hat{y}{ui})] ), where ( y{ui} = 1 ) for observed interactions and 0 otherwise. Since unobserved pairs vastly outnumber observed ones, negative sampling (e.g., 4 negatives per positive) keeps training feasible. The optimizer is Adam, with a learning rate of 0.001, balancing speed and stability. On the MovieLens 1M dataset, NeuMF achieved a Hit Ratio@10 of 0.71, beating MF’s 0.67 by 6%, thanks to its ability to model non-linear patterns. Compared to MLP alone (0.69), NeuMF’s fusion of GMF’s linearity and MLP’s depth proved superior. The special change—swapping a dot product for a neural network—unlocked this flexibility, though NCF ignores auxiliary features like user demographics and can’t model sequential behaviors.\nWide \u0026 Deep Learning (2016) NCF’s focus on user-item pairs left out contextual features and struggled with generalization in sparse, diverse settings. Enter Wide \u0026 Deep Learning, proposed by Cheng et al. at Google in 2016, designed for app recommendations on Google Play. The problem was twofold: linear models like logistic regression memorized specific patterns (e.g., “user installed app A”) but couldn’t generalize to unseen data, while DNNs generalized well but missed rare, critical interactions. Wide \u0026 Deep combined a linear “wide” model for memorization with a DNN “deep” model for generalization, aiming to balance both.\nThe architecture starts with inputs: sparse features (e.g., user ID, app ID) mapped to embeddings (e.g., 32 dimensions) and dense features (e.g., user age) fed raw. The wide component is a linear model: ( \\mathbf{y}{\\text{wide}} = \\mathbf{w}^T \\mathbf{x} + b ), where ( \\mathbf{x} ) includes raw features and hand-crafted cross-features (e.g., “user installed app A AND app B”), capturing low-order interactions. Designing these cross-features required domain expertise, a key modification over pure DNNs. The deep component is an MLP with three hidden layers (1024, 512, 256 neurons) using ReLU: ( \\mathbf{z}1 = \\text{ReLU}(\\mathbf{W}1 \\mathbf{e} + \\mathbf{b}1) ), where ( \\mathbf{e} ) concatenates embeddings and dense inputs, learning higher-order interactions. The outputs merge via a weighted sum: ( \\hat{y} = \\sigma(\\mathbf{w}{\\text{wide}}^T \\mathbf{y}{\\text{wide}} + \\mathbf{w}{\\text{deep}}^T \\mathbf{z}{\\text{deep}} + b) ), yielding a click probability.\nThe loss is logistic (binary cross-entropy), optimized differently per component: FTRL with L1 regularization for the wide part (encouraging sparsity) and AdaGrad for the deep part (adapting to dense gradients). On Google Play, Wide \u0026 Deep boosted app installations by 3.9% over a wide-only model and 1% over a deep-only model, proving the hybrid’s value. Unlike NCF, it leverages auxiliary features, but the manual engineering of cross-features limits scalability, and it doesn’t address sequential data or higher-order interactions beyond the wide part.\nDeepFM (2017) Wide \u0026 Deep’s reliance on manual feature engineering was a bottleneck, especially for large-scale systems with thousands of features. In 2017, Guo et al. introduced DeepFM, targeting CTR prediction in online advertising (e.g., Criteo dataset), by combining Factorization Machines (FM) with a DNN to automate feature interactions. FM’s strength was modeling pairwise interactions efficiently, but it missed higher-order patterns; DeepFM extended it to capture both low- and high-order interactions without human intervention.\nDeepFM’s inputs are sparse features (e.g., user ID, ad ID) mapped to embeddings ( \\mathbf{v}i \\in \\mathbb{R}^{10} ). The FM component computes: ( \\mathbf{y}{\\text{FM}} = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j ), capturing second-order interactions via dot products. The deep component, an MLP with three 200-neuron layers and ReLU, takes the same embeddings: ( \\mathbf{z}1 = \\text{ReLU}(\\mathbf{W}1 \\mathbf{v} + \\mathbf{b}1) ), learning higher-order interactions. Sharing embeddings between FM and DNN ensures consistency and efficiency—a key design choice. The final output combines both: ( \\hat{y} = \\sigma(\\mathbf{y}{\\text{FM}} + \\mathbf{w}{\\text{deep}}^T \\mathbf{z}{\\text{deep}}) ).\nThe loss is binary cross-entropy, optimized with Adam (learning rate 0.001). On Criteo, DeepFM hit an AUC of 0.801, edging out Wide \u0026 Deep (0.799) and FM (0.785), as it automated feature engineering while retaining FM’s strengths. This modification—replacing Wide \u0026 Deep’s manual cross-features with FM—made it scalable, though it still overlooks sequential user behaviors critical for dynamic settings like e-commerce.\nDeep Interest Network (DIN, 2017) DeepFM’s static modeling couldn’t capture how user interests evolve, say, during an e-commerce browsing session. Zhou et al. at Alibaba introduced the Deep Interest Network (DIN) in 2017 to address this, using an attention mechanism to weigh historical behaviors based on their relevance to a candidate item. Proposed for ad recommendations, DIN recognized that not all past interactions (e.g., clicked items) equally inform the next click, necessitating a dynamic approach.\nDIN’s inputs include a user’s behavior sequence ( S_u = {v_1, v_2, \\ldots, v_T} ) (e.g., clicked items), a candidate ad ( v_a ), and context features, all mapped to embeddings. The core innovation is the attention mechanism: for each historical item ( v_i ), it computes a weight ( \\alpha_i = f(\\mathbf{v}_i, \\mathbf{v}_a) ) using a small MLP: ( f(\\mathbf{v}_i, \\mathbf{v}_a) = \\text{ReLU}(\\mathbf{W} [\\mathbf{v}_i, \\mathbf{v}_a, \\mathbf{v}_i \\odot \\mathbf{v}_a] + \\mathbf{b}) ). This weights items by relevance, forming a user interest vector ( \\mathbf{s}u = \\sum{i=1}^T \\alpha_i \\mathbf{v}i ). This vector, the candidate embedding, and context embeddings feed into an MLP with three layers (200, 80, 2 neurons) and ReLU, ending in a sigmoid output: ( \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{z}{\\text{deep}}) ).\nThe loss is binary cross-entropy, optimized with Adam (learning rate 0.001). On Alibaba’s dataset, DIN’s AUC of 0.82 beat DeepFM’s 0.80 by 2%, highlighting attention’s power in sequential modeling. Unlike DeepFM, DIN adapts to temporal dynamics, but it’s tailored to single-task CTR prediction, not multi-objective scenarios.\nDeep Learning Recommendation Model (DLRM, 2019) DIN’s single-tower design wasn’t built for the massive scale and diverse features of systems like Facebook’s ad platform. In 2019, Naumov et al. proposed DLRM, a multi-tower architecture for CTR prediction, explicitly modeling pairwise interactions for scalability and interpretability. The need arose from handling billions of sparse features (e.g., ad IDs) alongside dense ones (e.g., user stats), where implicit modeling slowed training.\nDLRM’s inputs split into dense features (e.g., user age) and sparse features (e.g., user ID), mapped to embeddings. The dense tower is an MLP with three layers (512, 256, 128 neurons) and ReLU, processing continuous inputs. The sparse tower computes pairwise dot products between embeddings: ( \\mathbf{z}_{ij} = \\mathbf{v}_i^T \\mathbf{v}_j ), forming an interaction vector. These outputs concatenate with the dense tower’s result, feeding a top MLP (128, 1 neurons) with ReLU and sigmoid: ( \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{z}) ).\nThe loss is binary cross-entropy, optimized with Adam or SGD. On Criteo, DLRM’s AUC of 0.802 slightly topped DeepFM’s 0.801, with better scalability from its parallel towers—a key change over single-tower designs. However, it focuses on single-task CTR, not multi-task or sequential needs.\nAdaptive Task-to-Task Fusion (AdaTT, 2023) DLRM’s single-task focus couldn’t handle multi-objective RecSys, like predicting CTR and conversion rate (CVR) together. Multi-Task Learning (MTL) emerged to share representations across tasks, but task conflicts often hurt performance. Yang et al.’s AdaTT, introduced in 2023 at Kuaishou, tackled this with an adaptive task-to-task fusion network, dynamically balancing task interactions.\nAdaTT’s inputs—shared features (e.g., user ID, item ID)—map to embeddings feeding a shared bottom MLP: ( \\mathbf{z}_{\\text{shared}} = \\text{ReLU}(\\mathbf{W} \\mathbf{e} + \\mathbf{b}) ). Task-specific towers (e.g., CTR, CVR) process this into ( \\mathbf{z}t = \\text{MLP}t(\\mathbf{z}{\\text{shared}}) ). The innovation is an attention-based fusion: ( \\mathbf{z}t’ = \\sum{s \\neq t} \\alpha{ts} \\mathbf{z}s ), where ( \\alpha{ts} ) weights contributions from other tasks, computed via a task-to-task attention MLP. Outputs are per-task sigmoids: ( \\hat{y}_t = \\sigma(\\mathbf{w}_t^T \\mathbf{z}_t’) ).\nThe loss is a weighted sum: ( L = \\sum_t \\lambda_t L_t ) (binary cross-entropy per task), optimized with Adam. On Kuaishou’s dataset, AdaTT lifted CTR AUC by 1.5% and CVR AUC by 2% over single-task models, thanks to its adaptive fusion—a leap over static MTL. Its complexity, though, demands careful tuning.\nConclusion: What’s Next From NCF’s non-linear leap to AdaTT’s multi-task finesse, DNNs have reshaped RecSys, each model solving a prior limitation: NCF broke linearity, Wide \u0026 Deep merged memorization and generalization, DeepFM automated engineering, DIN added sequence, DLRM scaled up, and AdaTT tackled multiple goals. Part 4 will explore graph neural networks and transformers, pushing RecSys further into complex, real-time domains.\nReferences He, X., et al. (2017). Neural Collaborative Filtering. arXiv:1708.05031. Cheng, H.-T., et al. (2016). Wide \u0026 Deep Learning for Recommender Systems. arXiv:1606.07792. Guo, H., et al. (2017). DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. arXiv:1703.04247. Zhou, G., et al. (2017). Deep Interest Network for Click-Through Rate Prediction. arXiv:1706.06978. Naumov, M., et al. (2019). Deep Learning Recommendation Model for Personalization and Recommendation Systems. arXiv:1906.00091. Yang, S., et al. (2023). AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. arXiv:2304.04959. ","wordCount":"2218","inLanguage":"en","datePublished":"2025-03-12T00:00:00Z","dateModified":"2025-03-12T00:00:00Z","author":[{"@type":"Person","name":"Abdullah Al Mamun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://newabdullah.com/posts/the-evaluation-of-recsys-part-3/"},"publisher":{"@type":"Organization","name":"Abdullah Al Mamun","logo":{"@type":"ImageObject","url":"https://newabdullah.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://newabdullah.com/ accesskey=h title="Abdullah Al Mamun (Alt + H)">Abdullah Al Mamun</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><button class=menu-toggle onclick=toggleMenu()>☰</button><ul id=menu><li><a href=https://newabdullah.com/archives title=Blog><span>Blog</span></a></li><li><a href=https://newabdullah.com/about title="About Me"><span>About Me</span></a></li><li><a href=https://www.newabdullah.com/Abdullah_Resume.pdf title=Resume><span>Resume</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://newabdullah.com/gallery/ title=Gallery><span>Gallery</span></a></li><li><a href=https://newabdullah.com/contact title=Contact><span>Contact</span></a></li><li><a href=https://www.linkedin.com/in/newabdullah/ title=Linkedin><span><i class='fab fa-linkedin'></i>Linkedin</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://newabdullah.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">The Evaluation of RecSys - Part 3</h1><div class=post-meta><span title='2025-03-12 00:00:00 +0000 UTC'>March 12, 2025</span>&nbsp;·&nbsp;Abdullah Al Mamun</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#context-why-this-post-matters-who-its-for-and-what-youll-learn aria-label="Context: Why This Post Matters, Who It’s For, and What You’ll Learn">Context: Why This Post Matters, Who It’s For, and What You’ll Learn</a></li><li><a href=#recap-where-we-left-off aria-label="Recap: Where We Left Off">Recap: Where We Left Off</a></li><li><a href=#the-big-picture-the-deep-learning-revolution-in-recsys aria-label="The Big Picture: The Deep Learning Revolution in RecSys">The Big Picture: The Deep Learning Revolution in RecSys</a></li><li><a href=#deep-dive-the-evolution-of-dnns-in-recsys aria-label="Deep Dive: The Evolution of DNNs in RecSys">Deep Dive: The Evolution of DNNs in RecSys</a><ul><li><a href=#neural-collaborative-filtering-ncf-2017 aria-label="Neural Collaborative Filtering (NCF, 2017)">Neural Collaborative Filtering (NCF, 2017)</a></li><li><a href=#wide--deep-learning-2016 aria-label="Wide & Deep Learning (2016)">Wide & Deep Learning (2016)</a></li><li><a href=#deepfm-2017 aria-label="DeepFM (2017)">DeepFM (2017)</a></li><li><a href=#deep-interest-network-din-2017 aria-label="Deep Interest Network (DIN, 2017)">Deep Interest Network (DIN, 2017)</a></li><li><a href=#deep-learning-recommendation-model-dlrm-2019 aria-label="Deep Learning Recommendation Model (DLRM, 2019)">Deep Learning Recommendation Model (DLRM, 2019)</a></li><li><a href=#adaptive-task-to-task-fusion-adatt-2023 aria-label="Adaptive Task-to-Task Fusion (AdaTT, 2023)">Adaptive Task-to-Task Fusion (AdaTT, 2023)</a></li></ul></li><li><a href=#conclusion-whats-next aria-label="Conclusion: What’s Next">Conclusion: What’s Next</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=context-why-this-post-matters-who-its-for-and-what-youll-learn>Context: Why This Post Matters, Who It’s For, and What You’ll Learn<a hidden class=anchor aria-hidden=true href=#context-why-this-post-matters-who-its-for-and-what-youll-learn>#</a></h2><p>Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide & Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It’s tailored for data scientists, ML engineers, and tech professionals—particularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising—who need a deep, technical understanding of these advancements.</p><h2 id=recap-where-we-left-off>Recap: Where We Left Off<a hidden class=anchor aria-hidden=true href=#recap-where-we-left-off>#</a></h2><p>In Part 2, we saw how FM extended MF by modeling pairwise feature interactions, making it a powerhouse for sparse settings like click-through rate (CTR) prediction. Its prediction function, ( \hat{y}(\mathbf{x}) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle v_i, v_j \rangle x_i x_j ), captured second-order relationships efficiently but couldn’t handle higher-order interactions or non-linear patterns beyond its linear assumptions. XGBoost, meanwhile, leveraged tree ensembles to rank items based on non-linear feature combinations, shining in tasks like top-N recommendations. Yet, it struggled with high-dimensional sparse data and required extensive manual feature engineering, limiting its scalability. These gaps—missing deep non-linearities, higher-order interactions, and sequential modeling—paved the way for DNNs, which, starting in 2016, redefined RecSys by learning complex patterns directly from raw data.</p><h2 id=the-big-picture-the-deep-learning-revolution-in-recsys>The Big Picture: The Deep Learning Revolution in RecSys<a hidden class=anchor aria-hidden=true href=#the-big-picture-the-deep-learning-revolution-in-recsys>#</a></h2><p>Picture a recommendation system as a guide helping you navigate a vast library. In Part 2, our guide used simple rules: FM paired clues like your reading history with book traits, while XGBoost ranked options by studying everyone’s preferences. But what if your interests shift over time (say, from mysteries to sci-fi), or the guide needs to predict both what you’ll read and whether you’ll buy it? These earlier methods faltered. DNNs emerged as a smarter guide, capable of deciphering intricate patterns, tracking sequential behaviors, and juggling multiple goals. From 2016’s Wide & Deep to 2023’s AdaTT, this era saw RecSys evolve to handle complex user behaviors with unprecedented accuracy, shaping modern systems in companies like Google, Alibaba, and Facebook.</p><h2 id=deep-dive-the-evolution-of-dnns-in-recsys>Deep Dive: The Evolution of DNNs in RecSys<a hidden class=anchor aria-hidden=true href=#deep-dive-the-evolution-of-dnns-in-recsys>#</a></h2><p>Let’s explore this journey, starting with Neural Collaborative Filtering, which kicked off the DNN era by rethinking how we model user-item interactions.</p><h3 id=neural-collaborative-filtering-ncf-2017>Neural Collaborative Filtering (NCF, 2017)<a hidden class=anchor aria-hidden=true href=#neural-collaborative-filtering-ncf-2017>#</a></h3><p>Traditional MF, a staple from Part 1, predicts user-item interactions via a dot product: ( \hat{r}_{ui} = p_u^T q_i ), where ( p_u ) and ( q_i ) are latent vectors for user ( u ) and item ( i ). This worked well for explicit ratings but assumed linearity, missing non-linear patterns in implicit feedback like clicks or views. In 2017, He et al. proposed Neural Collaborative Filtering (NCF) to overcome this, replacing the dot product with a neural network to capture complex, non-linear relationships. The motivation was clear: real-world preferences aren’t linear—liking sci-fi movies doesn’t linearly predict liking sci-fi books—and DNNs, fresh from successes in vision and NLP, offered a way to model these nuances.</p><p>NCF’s architecture comes in three flavors. First, the inputs are simple: one-hot encoded user ID ( \mathbf{u} ) and item ID ( \mathbf{i} ), mapped to dense embeddings ( \mathbf{p}_u ) and ( \mathbf{q}_i \in \mathbb{R}^{32} ) via lookup tables. The Generalized Matrix Factorization (GMF) variant mimics MF but with a neural twist: it computes an element-wise product ( \mathbf{p}_u \odot \mathbf{q}<em>i ), feeds it through a linear layer with weights ( \mathbf{w} ), and applies a sigmoid activation to output a probability: ( \hat{y}</em>{ui} = \sigma(\mathbf{w}^T (\mathbf{p}_u \odot \mathbf{q}_i)) ). This retains MF’s linear interaction but learns the weighting neurally. The Multi-Layer Perceptron (MLP) variant takes a different tack, concatenating the embeddings into ( [\mathbf{p}_u, \mathbf{q}_i] ) and passing them through three fully connected layers (e.g., 256, 128, 64 neurons) with ReLU activations: ( \mathbf{z}_1 = \text{ReLU}(\mathbf{W}_1 [\mathbf{p}<em>u, \mathbf{q}<em>i] + \mathbf{b}<em>1) ), followed by more layers, ending in a prediction layer. This captures non-linear interactions unavailable to MF. Finally, Neural Matrix Factorization (NeuMF) combines both, concatenating GMF’s and MLP’s penultimate outputs and applying a final linear layer: ( \hat{y}</em>{ui} = \sigma(\mathbf{w}^T [\mathbf{z}</em>{\text{GMF}}, \mathbf{z}</em>{\text{MLP}}]) ). This hybrid leverages both linear and non-linear modeling.</p><p>For implicit feedback (e.g., clicks), NCF uses binary cross-entropy as its loss: ( L = -\sum_{(u,i) \in D} [y_{ui} \log(\hat{y}<em>{ui}) + (1-y</em>{ui}) \log(1-\hat{y}<em>{ui})] ), where ( y</em>{ui} = 1 ) for observed interactions and 0 otherwise. Since unobserved pairs vastly outnumber observed ones, negative sampling (e.g., 4 negatives per positive) keeps training feasible. The optimizer is Adam, with a learning rate of 0.001, balancing speed and stability. On the MovieLens 1M dataset, NeuMF achieved a Hit Ratio@10 of 0.71, beating MF’s 0.67 by 6%, thanks to its ability to model non-linear patterns. Compared to MLP alone (0.69), NeuMF’s fusion of GMF’s linearity and MLP’s depth proved superior. The special change—swapping a dot product for a neural network—unlocked this flexibility, though NCF ignores auxiliary features like user demographics and can’t model sequential behaviors.</p><h3 id=wide--deep-learning-2016>Wide & Deep Learning (2016)<a hidden class=anchor aria-hidden=true href=#wide--deep-learning-2016>#</a></h3><p>NCF’s focus on user-item pairs left out contextual features and struggled with generalization in sparse, diverse settings. Enter Wide & Deep Learning, proposed by Cheng et al. at Google in 2016, designed for app recommendations on Google Play. The problem was twofold: linear models like logistic regression memorized specific patterns (e.g., “user installed app A”) but couldn’t generalize to unseen data, while DNNs generalized well but missed rare, critical interactions. Wide & Deep combined a linear “wide” model for memorization with a DNN “deep” model for generalization, aiming to balance both.</p><p>The architecture starts with inputs: sparse features (e.g., user ID, app ID) mapped to embeddings (e.g., 32 dimensions) and dense features (e.g., user age) fed raw. The wide component is a linear model: ( \mathbf{y}<em>{\text{wide}} = \mathbf{w}^T \mathbf{x} + b ), where ( \mathbf{x} ) includes raw features and hand-crafted cross-features (e.g., “user installed app A AND app B”), capturing low-order interactions. Designing these cross-features required domain expertise, a key modification over pure DNNs. The deep component is an MLP with three hidden layers (1024, 512, 256 neurons) using ReLU: ( \mathbf{z}<em>1 = \text{ReLU}(\mathbf{W}<em>1 \mathbf{e} + \mathbf{b}<em>1) ), where ( \mathbf{e} ) concatenates embeddings and dense inputs, learning higher-order interactions. The outputs merge via a weighted sum: ( \hat{y} = \sigma(\mathbf{w}</em>{\text{wide}}^T \mathbf{y}</em>{\text{wide}} + \mathbf{w}</em>{\text{deep}}^T \mathbf{z}</em>{\text{deep}} + b) ), yielding a click probability.</p><p>The loss is logistic (binary cross-entropy), optimized differently per component: FTRL with L1 regularization for the wide part (encouraging sparsity) and AdaGrad for the deep part (adapting to dense gradients). On Google Play, Wide & Deep boosted app installations by 3.9% over a wide-only model and 1% over a deep-only model, proving the hybrid’s value. Unlike NCF, it leverages auxiliary features, but the manual engineering of cross-features limits scalability, and it doesn’t address sequential data or higher-order interactions beyond the wide part.</p><h3 id=deepfm-2017>DeepFM (2017)<a hidden class=anchor aria-hidden=true href=#deepfm-2017>#</a></h3><p>Wide & Deep’s reliance on manual feature engineering was a bottleneck, especially for large-scale systems with thousands of features. In 2017, Guo et al. introduced DeepFM, targeting CTR prediction in online advertising (e.g., Criteo dataset), by combining Factorization Machines (FM) with a DNN to automate feature interactions. FM’s strength was modeling pairwise interactions efficiently, but it missed higher-order patterns; DeepFM extended it to capture both low- and high-order interactions without human intervention.</p><p>DeepFM’s inputs are sparse features (e.g., user ID, ad ID) mapped to embeddings ( \mathbf{v}<em>i \in \mathbb{R}^{10} ). The FM component computes: ( \mathbf{y}</em>{\text{FM}} = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j ), capturing second-order interactions via dot products. The deep component, an MLP with three 200-neuron layers and ReLU, takes the same embeddings: ( \mathbf{z}<em>1 = \text{ReLU}(\mathbf{W}<em>1 \mathbf{v} + \mathbf{b}<em>1) ), learning higher-order interactions. Sharing embeddings between FM and DNN ensures consistency and efficiency—a key design choice. The final output combines both: ( \hat{y} = \sigma(\mathbf{y}</em>{\text{FM}} + \mathbf{w}</em>{\text{deep}}^T \mathbf{z}</em>{\text{deep}}) ).</p><p>The loss is binary cross-entropy, optimized with Adam (learning rate 0.001). On Criteo, DeepFM hit an AUC of 0.801, edging out Wide & Deep (0.799) and FM (0.785), as it automated feature engineering while retaining FM’s strengths. This modification—replacing Wide & Deep’s manual cross-features with FM—made it scalable, though it still overlooks sequential user behaviors critical for dynamic settings like e-commerce.</p><h3 id=deep-interest-network-din-2017>Deep Interest Network (DIN, 2017)<a hidden class=anchor aria-hidden=true href=#deep-interest-network-din-2017>#</a></h3><p>DeepFM’s static modeling couldn’t capture how user interests evolve, say, during an e-commerce browsing session. Zhou et al. at Alibaba introduced the Deep Interest Network (DIN) in 2017 to address this, using an attention mechanism to weigh historical behaviors based on their relevance to a candidate item. Proposed for ad recommendations, DIN recognized that not all past interactions (e.g., clicked items) equally inform the next click, necessitating a dynamic approach.</p><p>DIN’s inputs include a user’s behavior sequence ( S_u = {v_1, v_2, \ldots, v_T} ) (e.g., clicked items), a candidate ad ( v_a ), and context features, all mapped to embeddings. The core innovation is the attention mechanism: for each historical item ( v_i ), it computes a weight ( \alpha_i = f(\mathbf{v}_i, \mathbf{v}_a) ) using a small MLP: ( f(\mathbf{v}_i, \mathbf{v}_a) = \text{ReLU}(\mathbf{W} [\mathbf{v}_i, \mathbf{v}_a, \mathbf{v}_i \odot \mathbf{v}_a] + \mathbf{b}) ). This weights items by relevance, forming a user interest vector ( \mathbf{s}<em>u = \sum</em>{i=1}^T \alpha_i \mathbf{v}<em>i ). This vector, the candidate embedding, and context embeddings feed into an MLP with three layers (200, 80, 2 neurons) and ReLU, ending in a sigmoid output: ( \hat{y} = \sigma(\mathbf{w}^T \mathbf{z}</em>{\text{deep}}) ).</p><p>The loss is binary cross-entropy, optimized with Adam (learning rate 0.001). On Alibaba’s dataset, DIN’s AUC of 0.82 beat DeepFM’s 0.80 by 2%, highlighting attention’s power in sequential modeling. Unlike DeepFM, DIN adapts to temporal dynamics, but it’s tailored to single-task CTR prediction, not multi-objective scenarios.</p><h3 id=deep-learning-recommendation-model-dlrm-2019>Deep Learning Recommendation Model (DLRM, 2019)<a hidden class=anchor aria-hidden=true href=#deep-learning-recommendation-model-dlrm-2019>#</a></h3><p>DIN’s single-tower design wasn’t built for the massive scale and diverse features of systems like Facebook’s ad platform. In 2019, Naumov et al. proposed DLRM, a multi-tower architecture for CTR prediction, explicitly modeling pairwise interactions for scalability and interpretability. The need arose from handling billions of sparse features (e.g., ad IDs) alongside dense ones (e.g., user stats), where implicit modeling slowed training.</p><p>DLRM’s inputs split into dense features (e.g., user age) and sparse features (e.g., user ID), mapped to embeddings. The dense tower is an MLP with three layers (512, 256, 128 neurons) and ReLU, processing continuous inputs. The sparse tower computes pairwise dot products between embeddings: ( \mathbf{z}_{ij} = \mathbf{v}_i^T \mathbf{v}_j ), forming an interaction vector. These outputs concatenate with the dense tower’s result, feeding a top MLP (128, 1 neurons) with ReLU and sigmoid: ( \hat{y} = \sigma(\mathbf{w}^T \mathbf{z}) ).</p><p>The loss is binary cross-entropy, optimized with Adam or SGD. On Criteo, DLRM’s AUC of 0.802 slightly topped DeepFM’s 0.801, with better scalability from its parallel towers—a key change over single-tower designs. However, it focuses on single-task CTR, not multi-task or sequential needs.</p><h3 id=adaptive-task-to-task-fusion-adatt-2023>Adaptive Task-to-Task Fusion (AdaTT, 2023)<a hidden class=anchor aria-hidden=true href=#adaptive-task-to-task-fusion-adatt-2023>#</a></h3><p>DLRM’s single-task focus couldn’t handle multi-objective RecSys, like predicting CTR and conversion rate (CVR) together. Multi-Task Learning (MTL) emerged to share representations across tasks, but task conflicts often hurt performance. Yang et al.’s AdaTT, introduced in 2023 at Kuaishou, tackled this with an adaptive task-to-task fusion network, dynamically balancing task interactions.</p><p>AdaTT’s inputs—shared features (e.g., user ID, item ID)—map to embeddings feeding a shared bottom MLP: ( \mathbf{z}_{\text{shared}} = \text{ReLU}(\mathbf{W} \mathbf{e} + \mathbf{b}) ). Task-specific towers (e.g., CTR, CVR) process this into ( \mathbf{z}<em>t = \text{MLP}<em>t(\mathbf{z}</em>{\text{shared}}) ). The innovation is an attention-based fusion: ( \mathbf{z}<em>t&rsquo; = \sum</em>{s \neq t} \alpha</em>{ts} \mathbf{z}<em>s ), where ( \alpha</em>{ts} ) weights contributions from other tasks, computed via a task-to-task attention MLP. Outputs are per-task sigmoids: ( \hat{y}_t = \sigma(\mathbf{w}_t^T \mathbf{z}_t&rsquo;) ).</p><p>The loss is a weighted sum: ( L = \sum_t \lambda_t L_t ) (binary cross-entropy per task), optimized with Adam. On Kuaishou’s dataset, AdaTT lifted CTR AUC by 1.5% and CVR AUC by 2% over single-task models, thanks to its adaptive fusion—a leap over static MTL. Its complexity, though, demands careful tuning.</p><h2 id=conclusion-whats-next>Conclusion: What’s Next<a hidden class=anchor aria-hidden=true href=#conclusion-whats-next>#</a></h2><p>From NCF’s non-linear leap to AdaTT’s multi-task finesse, DNNs have reshaped RecSys, each model solving a prior limitation: NCF broke linearity, Wide & Deep merged memorization and generalization, DeepFM automated engineering, DIN added sequence, DLRM scaled up, and AdaTT tackled multiple goals. Part 4 will explore graph neural networks and transformers, pushing RecSys further into complex, real-time domains.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>He, X., et al. (2017). Neural Collaborative Filtering. <em>arXiv:1708.05031</em>.</li><li>Cheng, H.-T., et al. (2016). Wide & Deep Learning for Recommender Systems. <em>arXiv:1606.07792</em>.</li><li>Guo, H., et al. (2017). DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. <em>arXiv:1703.04247</em>.</li><li>Zhou, G., et al. (2017). Deep Interest Network for Click-Through Rate Prediction. <em>arXiv:1706.06978</em>.</li><li>Naumov, M., et al. (2019). Deep Learning Recommendation Model for Personalization and Recommendation Systems. <em>arXiv:1906.00091</em>.</li><li>Yang, S., et al. (2023). AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. <em>arXiv:2304.04959</em>.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://newabdullah.com/tags/recommendation-systems/>Recommendation Systems</a></li><li><a href=https://newabdullah.com/tags/deep-learning/>Deep Learning</a></li><li><a href=https://newabdullah.com/tags/neural-collaborative-filtering/>Neural Collaborative Filtering</a></li><li><a href=https://newabdullah.com/tags/wide-and-deep/>Wide and Deep</a></li><li><a href=https://newabdullah.com/tags/deepfm/>Deepfm</a></li><li><a href=https://newabdullah.com/tags/din/>Din</a></li><li><a href=https://newabdullah.com/tags/dlrm/>Dlrm</a></li><li><a href=https://newabdullah.com/tags/multi-task-learning/>Multi-Task Learning</a></li></ul><head><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css rel=stylesheet></head><div class=share-wrapper><button class=top-button>
<i class="fas fa-share-alt"></i> Share</button><div class=share-buttons><a href="https://twitter.com/share?url=https%3a%2f%2fnewabdullah.com%2fposts%2fthe-evaluation-of-recsys-part-3%2f&text=The%20Evaluation%20of%20RecSys%20-%20Part%203" target=_blank aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fnewabdullah.com%2fposts%2fthe-evaluation-of-recsys-part-3%2f" target=_blank aria-label="Share on Facebook"><i class="fab fa-facebook-f"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnewabdullah.com%2fposts%2fthe-evaluation-of-recsys-part-3%2f&title=The%20Evaluation%20of%20RecSys%20-%20Part%203" target=_blank aria-label="Share on LinkedIn"><i class="fab fa-linkedin-in"></i>
</a><a href="mailto:?subject=The%20Evaluation%20of%20RecSys%20-%20Part%203&body=https%3a%2f%2fnewabdullah.com%2fposts%2fthe-evaluation-of-recsys-part-3%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></div><div class=buy-me-a-coffee style=margin-top:1.5rem><a href=https://www.buymeacoffee.com/YOUR_USERNAME target=_blank aria-label="Support me with a coffee"><img src=https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png alt="Buy Me a Coffee" style=height:50px;border:0></a></div><div class=recent-posts style=margin-top:3rem><h2 class=headline>Recent Posts</h2><div style=display:flex;flex-wrap:wrap;gap:1rem><div style="border:1px solid #e0e0e0;border-radius:8px;padding:1rem;width:100%;max-width:250px"><a href=/posts/ai-agent/ style=text-decoration:none;color:inherit><h3 style="font-size:1.1rem;margin:0 0 .5rem">AI Agent with MCP: Smart Meeting Scheduler</h3><p style=font-size:.85rem;color:#666>Apr 11, 2025</p></a></div><div style="border:1px solid #e0e0e0;border-radius:8px;padding:1rem;width:100%;max-width:250px"><a href=/posts/the-evaluation-of-recsys-part2/ style=text-decoration:none;color:inherit><h3 style="font-size:1.1rem;margin:0 0 .5rem">The Evaluation of RecSys - Part 2</h3><p style=font-size:.85rem;color:#666>Mar 11, 2025</p></a></div></div></div></footer><script src=https://giscus.app/client.js data-repo=salahuddinjony/newabdullah.com data-repo-id=R_kgDOOVz2cg data-category=General data-category-id=DIC_kwDOOVz2cs4Co_xs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=dark data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>© [newabdullah 2025]</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>function toggleMenu(){document.getElementById("menu").classList.toggle("show")}</script></body></html>